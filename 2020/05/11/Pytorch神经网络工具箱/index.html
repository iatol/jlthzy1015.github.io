<!DOCTYPE html><html lang="zh-CN"><!--
             -. .                                        
       _____   ',' ,                                    
     ,'     ,'   ', ',                                  
   ,'     ,'      |  |                                  
   \       \       |  |                                  
     \ /^\   \    ,' ,'                                  
           \   \ ,' ,'      L'Internationale,            
     / ~-.___\.-'  ,'            Sera le genre humain.   
   /   .______.- ~ \                                     
 /   /'          \   \                                   
 \./               \/'                                   
                                                         
--><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Jlthzy"><title>Pytorch神经网络工具箱 · Jlthzy's Blog</title><meta name="description" content="在开始之前假装已经知道了Pytorch的数据结构和自动求导机制。
本文档在不断优化补充中。。。
七牛云+PicGo搭建属于自己的免费图床！！简直不要太方便！！！
1.核心组件
层：神经网络基本结构，将输入张量转换为输出张量

模型：神经网络框架（层的集合，inceptionV3等）

损失函数：参数"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style-dark.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/js/valine.min.js"></script><meta name="generator" content="Hexo 4.2.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><span class="donate-address">唯一指定邮箱：Jlthzy123@gmail.com</span><div id="stage" class="container"><div class="row"><div id="side-bar" class="col-sm-3 col-xs-12 side-container invisible"><div class="vertical-text site-title"><h3 tabindex="-1" class="site-title-small"><a href="/" class="a-title">当我拥着光亮 ⭐ 环着希望</a></h3><h1 tabindex="-1" class="site-title-large"><a href="/" class="a-title">就是这样 </a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div id="site-nav" class="site-title-links"><ul><li><a href="/">首页</a></li><li><a href="/archives">归档</a></li><li><a href="/tags">标签</a></li><li><a href="/categories/index.html"></a></li><li><a href="/abouts/index.html">关于</a></li><li><a href="/tags/index.html"></a></li><li><a href="/archives/index.html"></a></li><li class="soc"><a href="https://github.com/jlthzy1015" target="_blank" rel="noopener noreferrer"><i class="fa fa-github">&nbsp;</i></a><a href="https://twitter.com/xxxxx" target="_blank" rel="noopener noreferrer"><i class="fa fa-twitter">&nbsp;</i></a><a href="https://www.instagram.com/xxxxx" target="_blank" rel="noopener noreferrer"><i class="fa fa-instagram">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2020&nbsp;<a target="_blank" href="https://leohu.me" rel="noopener noreferrer">Jlthzy</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div id="main-container" class="col-sm-9 col-xs-12 main-container invisible"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><h1 class="post-title">Pytorch神经网络工具箱</h1><p class="post-meta"><span class="date meta-item">发布于&nbsp;2020-05-11</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a href="/tags/dl/" title="dl" class="a-tag">dl</a><span>&nbsp;</span></span></p><p class="post-abstract"><p>在开始之前假装已经知道了Pytorch的数据结构和自动求导机制。</p>
<p>本文档在不断优化补充中。。。</p>
<p>七牛云+PicGo搭建属于自己的免费图床！！简直不要太方便！！！</p>
<h2 id="1-核心组件"><a href="#1-核心组件" class="headerlink" title="1.核心组件"></a>1.核心组件</h2><ul>
<li><p>层：神经网络基本结构，将输入张量转换为输出张量</p>
</li>
<li><p>模型：神经网络框架（层的集合，inceptionV3等）</p>
</li>
<li><p>损失函数：参数学习的目标函数，最小化损失函数来学习各种参数</p>
</li>
<li><p>优化器：根据损失函数调整权重。</p>
<p>这些核心组件往往是互相影响的。</p>
</li>
</ul>
<h2 id="2-Pytorch的几个包介绍"><a href="#2-Pytorch的几个包介绍" class="headerlink" title="2.Pytorch的几个包介绍"></a>2.Pytorch的几个包介绍</h2><h3 id="2-1-torchvision"><a href="#2-1-torchvision" class="headerlink" title="2.1 torchvision"></a>2.1 torchvision</h3><p>这玩意儿有四个功能模块：model、datasets、transforms和utils</p>
<h4 id="2-1-1-transforms"><a href="#2-1-1-transforms" class="headerlink" title="2.1.1 transforms"></a>2.1.1 transforms</h4><p>主要功能是对源数据进行预处理，增强等等。transforms提供了对PIL(Python Imaging Library，是Python平台上的一个图像处理标准库) Image和Tensor对象的常用操作。</p>
<p><strong>PIL image</strong></p>
<blockquote>
<p>Scale/Resize:调整尺寸，长宽比保持不变</p>
</blockquote>
<p><strong>Tensor</strong></p>
<blockquote>
<p>teslkfdnmsekl</p>
</blockquote>
<h3 id="2-2-utils-data"><a href="#2-2-utils-data" class="headerlink" title="2.2 utils.data"></a>2.2 utils.data</h3><p>这玩意儿包括Dataset、DataLoader、random_split和*sampler。</p>
<p>DataLoader:定义一个新的迭代器，实现批量（batch）读取，打乱数据（shuffle），并提供并行加速功能。</p>
<h3 id="2-x-乱七八糟"><a href="#2-x-乱七八糟" class="headerlink" title="2.x 乱七八糟"></a>2.x 乱七八糟</h3><p><strong>to(device)</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mytensor = my_tensor.to(device)</span><br></pre></td></tr></table></figure>

<p>经常会用到to(device)操作，这行代码的意思是将所有最开始读取数据时的tensor变量copy一份到device所指定的GPU上去，之后的运算都在GPU上进行。</p>
<h2 id="实战Mnist"><a href="#实战Mnist" class="headerlink" title="实战Mnist"></a>实战Mnist</h2><p>数据预处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 导入 pytorch 内置的 mnist 数据</span></span><br><span class="line"><span class="keyword">from</span> torchvision.datasets <span class="keyword">import</span> mnist </span><br><span class="line"></span><br><span class="line"><span class="comment">#import torchvision</span></span><br><span class="line"><span class="comment">#导入预处理模块</span></span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment">#导入nn及优化器</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一些超参数</span></span><br><span class="line">train_batch_size = <span class="number">64</span></span><br><span class="line">test_batch_size = <span class="number">128</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_epoches = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义预处理函数</span></span><br><span class="line"><span class="comment">#把数据转成tensor，再归一化均值</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([<span class="number">0.5</span>], [<span class="number">0.5</span>])])</span><br><span class="line"><span class="comment">#下载数据，并对数据进行预处理</span></span><br><span class="line">train_dataset = mnist.MNIST(<span class="string">'./data'</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span>)</span><br><span class="line">test_dataset = mnist.MNIST(<span class="string">'./data'</span>, train=<span class="literal">False</span>, transform=transform)</span><br><span class="line"><span class="comment">#得到一个生成器</span></span><br><span class="line"><span class="comment">#训练集打撒，测试集不用</span></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>可视化源数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">examples = enumerate(test_loader)</span><br><span class="line">batch_idx, (example_data, example_targets) = next(examples)</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">6</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(example_data[i][<span class="number">0</span>], cmap=<span class="string">'gray'</span>, interpolation=<span class="string">'none'</span>)</span><br><span class="line">    plt.title(<span class="string">"Ground Truth: &#123;&#125;"</span>.format(example_targets[i]))</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br></pre></td></tr></table></figure>

<p><img src="http://qa1ryxpcn.bkt.clouddn.com/img/20051101.png" alt="Result"></p>
<p>构建模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用sequential构建网络，Sequential()函数的功能是将网络的层组合到一起</span></span><br><span class="line"><span class="string">    in_dim:输入图像的维度</span></span><br><span class="line"><span class="string">    out_dim:输出图像的维度</span></span><br><span class="line"><span class="string">    BatchNorm1d：P标准化</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, n_hidden_1, n_hidden_2, out_dim)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1),nn.BatchNorm1d(n_hidden_1))</span><br><span class="line">        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2),nn.BatchNorm1d(n_hidden_2))</span><br><span class="line">        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, out_dim))</span><br><span class="line">        </span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.layer1(x))</span><br><span class="line">        x = F.relu(self.layer2(x))</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line">momentum = <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#实例化模型</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">model = Net(<span class="number">28</span> * <span class="number">28</span>, <span class="number">300</span>, <span class="number">100</span>, <span class="number">10</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化器</span></span><br><span class="line"><span class="comment">#交叉熵</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment">#学习率和动量</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"></span><br><span class="line">print(optimizer)</span><br></pre></td></tr></table></figure>

<p>训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开始训练</span></span><br><span class="line">losses = []</span><br><span class="line">acces = []</span><br><span class="line">eval_losses = []</span><br><span class="line">eval_acces = []</span><br><span class="line">writer = SummaryWriter(log_dir=<span class="string">'logs'</span>,comment=<span class="string">'train-loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epoches):</span><br><span class="line">    train_loss = <span class="number">0</span></span><br><span class="line">    train_acc = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="comment">#每5个epoch就修改一次学习率</span></span><br><span class="line">    <span class="keyword">if</span> epoch%<span class="number">5</span>==<span class="number">0</span>:</span><br><span class="line">        optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>]*=<span class="number">0.9</span></span><br><span class="line">        print(optimizer.param_groups[<span class="number">0</span>][<span class="string">'lr'</span>])</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> img, label <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment">#复制到GPU设备</span></span><br><span class="line">        img=img.to(device)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        <span class="comment">#将向量拉直展平输入全连接层</span></span><br><span class="line">        img = img.view(img.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># 前向传播，此处的out相当于y，最后计算损失值</span></span><br><span class="line">        out = model(img)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        <span class="comment"># 反向传播，损失是累积的，所以需要清零，再进行反向传播</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 记录误差</span></span><br><span class="line">        train_loss += loss.item()</span><br><span class="line">        <span class="comment"># 保存loss的数据与epoch数值</span></span><br><span class="line">        writer.add_scalar(<span class="string">'Train'</span>, train_loss/len(train_loader), epoch)</span><br><span class="line">        <span class="comment"># 计算分类的准确率</span></span><br><span class="line">        <span class="comment">#取第一个维度下标最大值</span></span><br><span class="line">        _, pred = out.max(<span class="number">1</span>)</span><br><span class="line">        num_correct = (pred == label).sum().item()</span><br><span class="line">        acc = num_correct / img.shape[<span class="number">0</span>]</span><br><span class="line">        train_acc += acc</span><br><span class="line">        </span><br><span class="line">    losses.append(train_loss / len(train_loader))</span><br><span class="line">    acces.append(train_acc / len(train_loader))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在测试集上检验效果</span></span><br><span class="line">    eval_loss = <span class="number">0</span></span><br><span class="line">    eval_acc = <span class="number">0</span></span><br><span class="line">    <span class="comment">#net.eval() # 将模型改为预测模式</span></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">for</span> img, label <span class="keyword">in</span> test_loader:</span><br><span class="line">        img=img.to(device)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        img = img.view(img.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = model(img)</span><br><span class="line">        loss = criterion(out, label)</span><br><span class="line">        <span class="comment"># 记录误差</span></span><br><span class="line">        eval_loss += loss.item()</span><br><span class="line">        <span class="comment"># 记录准确率</span></span><br><span class="line">        _, pred = out.max(<span class="number">1</span>)</span><br><span class="line">        num_correct = (pred == label).sum().item()</span><br><span class="line">        acc = num_correct / img.shape[<span class="number">0</span>]</span><br><span class="line">        eval_acc += acc</span><br><span class="line">        </span><br><span class="line">    eval_losses.append(eval_loss / len(test_loader))</span><br><span class="line">    eval_acces.append(eval_acc / len(test_loader))</span><br><span class="line">    print(<span class="string">'epoch: &#123;&#125;, Train Loss: &#123;:.4f&#125;, Train Acc: &#123;:.4f&#125;, Test Loss: &#123;:.4f&#125;, Test Acc: &#123;:.4f&#125;'</span></span><br><span class="line">          .format(epoch, train_loss / len(train_loader), train_acc / len(train_loader), </span><br><span class="line">                     eval_loss / len(test_loader), eval_acc / len(test_loader)))</span><br></pre></td></tr></table></figure>

<p><img src="http://qa1ryxpcn.bkt.clouddn.com/img/20051102.png" alt="result"></p>
<p>可视化损失函数以及正确率图像</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(np.arange(len(losses)), losses, label=<span class="string">'Train Loss'</span>)</span><br><span class="line">plt.plot(np.arange(len(eval_losses)), eval_losses, label=<span class="string">'Test Loss'</span> )</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'Loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#准确率</span></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(np.arange(len(acces)), acces, label=<span class="string">'Train Acc'</span>)</span><br><span class="line">plt.plot(np.arange(len(eval_acces)), eval_acces, label=<span class="string">'Test Acc'</span> )</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'Acc'</span>)</span><br></pre></td></tr></table></figure>

<p><img src="http://qa1ryxpcn.bkt.clouddn.com/img/20051103.png" alt="loss"></p>
<p><img src="http://qa1ryxpcn.bkt.clouddn.com/img/20051104.png" alt="acc"></p>
<h2 id="不同种类优化器比较"><a href="#不同种类优化器比较" class="headerlink" title="不同种类优化器比较"></a>不同种类优化器比较</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">EPOCH = <span class="number">12</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 生成训练数据</span></span><br><span class="line"><span class="comment"># torch.unsqueeze() 的作用是将一维变二维，torch只能处理二维的数据</span></span><br><span class="line">x = torch.unsqueeze(torch.linspace(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1000</span>), dim=<span class="number">1</span>)  </span><br><span class="line"><span class="comment"># 0.1 * torch.normal(x.size())增加噪点</span></span><br><span class="line">y = x.pow(<span class="number">2</span>) + <span class="number">0.1</span> * torch.normal(torch.zeros(*x.size()))</span><br><span class="line"></span><br><span class="line">torch_dataset = Data.TensorDataset(x,y)</span><br><span class="line"><span class="comment">#得到一个代批量的生成器</span></span><br><span class="line">loader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net2</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net2, self).__init__()</span><br><span class="line">        self.hidden = torch.nn.Linear(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line">        self.predict = torch.nn.Linear(<span class="number">20</span>, <span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 前向传递</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.hidden(x))</span><br><span class="line">        x = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"> </span><br><span class="line">net_SGD = Net2()</span><br><span class="line">net_Momentum = Net2()</span><br><span class="line">net_RMSProp = Net2()</span><br><span class="line">net_Adam = Net2()</span><br><span class="line"> </span><br><span class="line">nets = [net_SGD, net_Momentum, net_RMSProp, net_Adam]</span><br><span class="line"> </span><br><span class="line">opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line">opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.9</span>)</span><br><span class="line">opt_RMSProp = torch.optim.RMSprop(net_RMSProp.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line">opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br><span class="line">optimizers = [opt_SGD, opt_Momentum, opt_RMSProp, opt_Adam]</span><br><span class="line"> </span><br><span class="line">loss_func = torch.nn.MSELoss()</span><br><span class="line"> </span><br><span class="line">loss_his = [[], [], [], []]  <span class="comment"># 记录损失</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCH):</span><br><span class="line">    <span class="keyword">for</span> step, (batch_x, batch_y) <span class="keyword">in</span> enumerate(loader):</span><br><span class="line">        <span class="keyword">for</span> net, opt,l_his <span class="keyword">in</span> zip(nets, optimizers, loss_his):</span><br><span class="line">            output = net(batch_x)  <span class="comment"># get output for every net</span></span><br><span class="line">            loss = loss_func(output, batch_y)  <span class="comment"># compute loss for every net</span></span><br><span class="line">            opt.zero_grad()  <span class="comment"># clear gradients for next train</span></span><br><span class="line">            loss.backward()  <span class="comment"># backpropagation, compute gradients</span></span><br><span class="line">            opt.step()  <span class="comment"># apply gradients</span></span><br><span class="line">            l_his.append(loss.data.numpy())  <span class="comment"># loss recoder</span></span><br><span class="line">            </span><br><span class="line">labels = [<span class="string">'SGD'</span>, <span class="string">'Momentum'</span>, <span class="string">'RMSprop'</span>, <span class="string">'Adam'</span>]</span><br><span class="line">plt.figure(figsize=(<span class="number">16</span>,<span class="number">9</span>))</span><br><span class="line"><span class="keyword">for</span> i, l_his <span class="keyword">in</span> enumerate(loss_his):</span><br><span class="line">    plt.plot(l_his, label=labels[i])</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Steps'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.ylim((<span class="number">0</span>, <span class="number">0.2</span>)) <span class="comment">#y轴的范围是0-0.2</span></span><br></pre></td></tr></table></figure>

<p><img src="http://qa1ryxpcn.bkt.clouddn.com/img/20051106.png" alt="result"></p>
</p></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2020/05/14/%E5%85%B3%E4%BA%8E%E8%84%91%E7%9A%84%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="视觉皮层基本概念"><i class="fa fa-angle-double-left"></i>&nbsp;上一篇: 视觉皮层基本概念</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2020/04/25/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%84%91%E8%AE%A4%E7%9F%A5week3%E4%B8%8A%E6%9C%BA/" title="EEG脑电数据预处理(Week3上机实验)">下一篇: EEG脑电数据预处理(Week3上机实验)&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div><a id="comments"></a><div id="valine-container"></div><script>(function(){
    if(typeof Valine !== 'undefined'){
        new Valine({
            el:'#valine-container',
            appId: '4BnPmxhNaSW5zalGBEpEWBkU-gzGzoHsz',
            appKey: 'J0kuzWw1GHsnqBOrvRjYGUjL',
            path: window.location.pathname
        })
    }
}())
</script></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2020&nbsp;<a target="_blank" href="https://leohu.me" rel="noopener noreferrer">Jlthzy</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.min.js"></script></body></html>